{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1."
      ],
      "metadata": {
        "id": "KQ-6NkABMvml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYM8sqvzUosd"
      },
      "outputs": [],
      "source": [
        "# Importing numpy library\n",
        "import numpy as np\n",
        "\n",
        "def generate_input_data(N, M, D):\n",
        "    \"\"\"\n",
        "    Function to generate input data matrix X for regression.\n",
        "\n",
        "    Parameters in the function:\n",
        "    N: Number of samples, is an integer.\n",
        "    M: Number of columns, is an integer.\n",
        "    D: Number of dimensions, is an integer.\n",
        "\n",
        "    The function returns input data matrix X, which is an n-dimensional array.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generates random data matrix of size NxM using numpy\n",
        "    sample_array = np.random.randn(N, M)\n",
        "\n",
        "    if(M==D):\n",
        "      # Generates an identity generatror-matrix of size MxM\n",
        "      S = np.identity(M)\n",
        "    else:\n",
        "      # Generates an generator-matrix of size MxD\n",
        "      S = np.random.rand(M, D)\n",
        "    # Generates input data matrix of size NxD by matrix-multiplication of sample_array and S matrices.\n",
        "    X = np.dot(sample_array, S)\n",
        "    return X\n",
        "\n",
        "# N = int(input(\"Input number of samples:\")) # Takes number of samples as input from the user\n",
        "# M = int(input(\"Input number of columns:\")) # Takes number of columns as input from the user\n",
        "# D = int(input(\"Input number of dimensions:\")) # Takes number of dimensions as input from the user\n",
        "N = 10000\n",
        "M = 10\n",
        "D = 5\n",
        "X = generate_input_data(N, M, D)\n",
        "\n",
        "print(\"Input data matrix X:\")\n",
        "print(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2"
      ],
      "metadata": {
        "id": "46PDAwv0NkX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used the formula:\n",
        "\n",
        "$t = w^T x + Ïµ (noise)$\n",
        "\n",
        "$y = w^T x$"
      ],
      "metadata": {
        "id": "nHJew2ywN9Ha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sigma = 0.3 #Defining variance\n",
        "\n",
        "w = np.ones((D+1,1)) #Initialising weight vector\n",
        "\n",
        "def gen_target_vector(X, w, sigma):\n",
        "  \"\"\"\n",
        "  The function generates target vector (Actual-Y values) with the inputs:\n",
        "  X: input data matrix\n",
        "  w: weight vector\n",
        "  sigma: variance\n",
        "  \"\"\"\n",
        "  t = np.dot(X, w) + np.random.normal(0, np.sqrt(sigma), size = (X.shape[0], 1)) # Generates the target vector\n",
        "  return t\n",
        "\n",
        "target = gen_target_vector(X, w[:-1], sigma)\n",
        "print(target)"
      ],
      "metadata": {
        "id": "qhuol6QuNiUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3."
      ],
      "metadata": {
        "id": "qZBLLWwGPhzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing time library\n",
        "import time\n",
        "\n",
        "# Importing matplot library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialising an array to store the time taken to calculate the pseudo-inverse with different sample size inpur data matrix.\n",
        "time_taken = []\n",
        "\n",
        "for i in range(N):\n",
        "    ith_sample_X = generate_input_data(i, M, 10)\n",
        "    start_time = time.time()\n",
        "    x_pseudo_inv = np.linalg.pinv(ith_sample_X)\n",
        "    end_time = time.time()\n",
        "    time_taken.append(end_time - start_time)\n",
        "\n",
        "x = np.linspace(1, 10000, 10000)\n",
        "plt.plot(x, time_taken,'o', markersize=1)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title('Process Time vs. Sample Size')\n",
        "plt.xlabel('Number of samples')\n",
        "plt.ylabel('Time taken by the process')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jTyRlDVuOkPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4."
      ],
      "metadata": {
        "id": "r9DkiSWRRVai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nrmse(target_vector, y_pred):\n",
        "    \"\"\"\n",
        "    This function returns the Noramalised Root Mean Squared Error using the input:\n",
        "    target_vector: Th actual Y-values vector\n",
        "    y_pred: The prediction matrix\n",
        "    \"\"\"\n",
        "    rmse = np.sqrt(np.mean((target_vector - y_pred)**2)) # Calculate the Root Mean Sqared Error.\n",
        "\n",
        "    range_values = np.max(target_vector) - np.min(target_vector) # Calculates the difference between the maximum and minimum values in target_vector.\n",
        "\n",
        "    nrmse_value = rmse / range_values # Calculates the NMRSE value.\n",
        "\n",
        "    return nrmse_value"
      ],
      "metadata": {
        "id": "Dffr5ioMQijQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5"
      ],
      "metadata": {
        "id": "2S3ndhLVRqXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_mean_square_error(X, w, true_vec):\n",
        "    \"\"\"\n",
        "    The function generates the graident of the Mean Square Error using inputs:\n",
        "    X: input data matrix\n",
        "    w: weight vector\n",
        "    true_vec: true observed values (randomly generated but fixed)\n",
        "    \"\"\"\n",
        "    # Calculates the predicted vector using X and w.\n",
        "    pred_vec = np.dot(X, w)\n",
        "    # Calculates the gradient of the Mean Square Error\n",
        "    gradient = -2*np.dot(X.T, (true_vec - pred_vec))/len(pred_vec)\n",
        "\n",
        "    # Returns the gradient of the Mean Square Error.\n",
        "    return gradient\n",
        "\n",
        "\n",
        "grad_mse = gradient_mean_square_error(X, w[:-1], target) # Calculating the gradient of the Mean Square Error\n",
        "\n",
        "print(f\"Gradient of Mean Square Error (MSE): {grad_mse}\") #Printing the gradient value\n",
        "print(grad_mse.shape)"
      ],
      "metadata": {
        "id": "H4aV91mCRomt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 6."
      ],
      "metadata": {
        "id": "y706lBHRSVRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L2_norm(w):\n",
        "    \"\"\"\n",
        "    This function returns the L2 norm with the inputs:\n",
        "    w: weight vector\n",
        "    \"\"\"\n",
        "    norm_L2 = np.linalg.norm(w, ord = 2) # Calculates the norm using the function np.linalg.norm.\n",
        "\n",
        "    grad_L2 = w/norm_L2\n",
        "    return grad_L2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "94bTleIfSCn1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 7"
      ],
      "metadata": {
        "id": "Fd3x-8xQTe6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L1_norm(w):\n",
        "    \"\"\"\n",
        "    This function returns the L1 norm with the inputs:\n",
        "    w: weight vector\n",
        "    \"\"\"\n",
        "    norm_L1 = np.linalg.norm(w, ord = 1) # Calculates the norm using the function np.linalg.norm.\n",
        "\n",
        "    grad_L1 = w/norm_L1\n",
        "    return grad_L1\n"
      ],
      "metadata": {
        "id": "4a4vn1KBTDt1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 8"
      ],
      "metadata": {
        "id": "FOZQyajxTg4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(pred_y,target_vector):\n",
        "  \"\"\"\n",
        "  This function calculates the Mean Squared Error using the inputs:\n",
        "  pred_y: the predicted values matrix\n",
        "  target_vector: the matrix containing the actual Y-values\n",
        "  \"\"\"\n",
        "  # print(pred_y.shape, target_vector.shape)\n",
        "  mse = np.mean((target_vector - pred_y)**2)\n",
        "  return mse"
      ],
      "metadata": {
        "id": "z5EyfhSDUcwT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent_wts(x, t, w, alpha, lambda_1, lambda_2):\n",
        "  \"\"\"\n",
        "  This function returns the updated weights vector using gradient descent and updated Mean Sqaured Error using the mse function defined above, with the inputs:\n",
        "  x: input data matrix\n",
        "  t: target vector containing the actual Y-values\n",
        "  w: weights vector\n",
        "  alpha: learning rate\n",
        "  lambda1 and lambda2: regularisation parameter\n",
        "  \"\"\"\n",
        "  wts_new = w - alpha * (gradient_mean_square_error(x, w, t) + lambda_1 * L1_norm(w) + lambda_2 * L2_norm(w)) # This line of code updates the weights vector.\n",
        "\n",
        "  y = np.dot(x, wts_new) # Calculates the new Y matrix using the updated weights.\n",
        "\n",
        "  mse_new = mse(y,t) # Calculates the updated MSE using the updated Y matrix.\n",
        "  # print(mse)\n",
        "  #print(eta * (gradient_mean_square_error(x, w, t) + lambda_1 * L1_norm(w) + lambda_2 * L2_norm(w)))\n",
        "  return wts_new, mse_new"
      ],
      "metadata": {
        "id": "TmgW1dewTcyk"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_descent(x,t,w,eta,lambda_1, lambda_2, max_iter, min_change_MSE):\n",
        "  \"\"\"\n",
        "  This function returns the updated weights vector and updated MSE value after iterating it through multiple times to get the MSE error below\n",
        "  a threshold value which implies a close-to-zero slope using the inputs:\n",
        "  x: input data matrix\n",
        "  t: target vector containing the actual Y-values\n",
        "  w: weights vector\n",
        "  alpha: learning rate\n",
        "  lambda1 and lambda2: regularisation parameter\n",
        "  max_iter: maximum number of iterations\n",
        "  min_change_MSE: the threshold MSE value below which the loop stops and we reach a close-to-zero slope.\n",
        "  \"\"\"\n",
        "  mse_new = 100 # Dummy MSE\n",
        "\n",
        "  wts_new = w.copy() # Makes a copy of the updated weights after every iteration\n",
        "\n",
        "  mse_old = 0 # Dummy MSE\n",
        "\n",
        "  diff = 0 # Holds the difference between the mse_new and mse_old.\n",
        "\n",
        "  for i in range(max_iter): # Loops around maximum iterations\n",
        "    if diff <= min_change_MSE and i > 1: # Stops loop if difference between last and current NRMSE is less than the threshold value.\n",
        "      break\n",
        "    p_new = gradient_descent_wts(x, t, wts_new, eta, lambda_1, lambda_2)\n",
        "    wts_new = p_new[0]\n",
        "    mse_new = p_new[1]\n",
        "\n",
        "    diff = abs(mse_new - mse_old)\n",
        "\n",
        "    mse_old = mse_new\n",
        "\n",
        "  return wts_new, mse_new"
      ],
      "metadata": {
        "id": "MaOSMwvjT6WA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9."
      ],
      "metadata": {
        "id": "TjxQmVwfja_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def NRMSE(x, t, w, eta, lambda_1, lambda_2, max_iter, min_change_NRMSE):\n",
        "  \"\"\"\n",
        "  This function returns the updated weights vector and updated MSE value after iterating it through multiple times to get the NRMSE error below\n",
        "  a threshold value which implies a close-to-zero slope using the inputs:\n",
        "  x: input data matrix\n",
        "  t: target vector containing the actual Y-values\n",
        "  w: weights vector\n",
        "  alpha: learning rate\n",
        "  lambda1 and lambda2: regularisation parameter\n",
        "  max_iter: maximum number of iterations\n",
        "  min_change_NRMSE: the threshold NRMSE value below which the loop stops and we reach a close-to-zero slope.\n",
        "\n",
        "  \"\"\"\n",
        "  mse_new = 100 # Dummy\n",
        "\n",
        "  wts_new = w.copy() # Makes a copy of the updated weights after every iteration\n",
        "\n",
        "  std_t = np.std(t) # Standard deviation for target matrix, for NRMSE calculation\n",
        "\n",
        "  nrmse_old = 0\n",
        "\n",
        "  diff = 0 # Holds the difference between the nrmse_new and nrmse_old.\n",
        "\n",
        "  for i in range(max_iter) :# Loops around maximum iterations\n",
        "    if diff <= min_change_NRMSE and i > 1: # Stops loop if difference between last and current NRMSE is less than the threshold value.\n",
        "      break\n",
        "    final_wts, mse_new = gradient_descent_wts(x,t,wts_new,eta,lambda_1, lambda_2)\n",
        "\n",
        "    nrmse_new = np.sqrt(mse_new) / std_t\n",
        "    diff = abs(nrmse_new - nrmse_old)\n",
        "    nrmse_old = nrmse_new\n",
        "\n",
        "\n",
        "  return final_wts, nrmse_new"
      ],
      "metadata": {
        "id": "JJImnLuTYKiE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMporting the train_test_split function to split the input sample into trainging data and test data.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sigma = [0.5, 1, 1.5, 2, 2.5] # Stores the various variance values\n",
        "sigma_impact = [] # Stores the change in\n",
        "\n",
        "for i in sigma:\n",
        "  target_i = gen_target_vector(X, w[:-1], i)\n",
        "\n",
        "  X_train, X_test, t_train, t_test = train_test_split(X, target_i, test_size=0.33, random_state=42)\n",
        "\n",
        "  wts_new, nrmse = NRMSE(X_train, t_train, w[:-1], 1e-2, 1e-1, 1e-1, 10000,1e-5)\n",
        "\n",
        "  sigma_impact.append(nrmse)\n",
        "\n",
        "for i in range(len(sigma)):\n",
        "  print(f\"When variance is {sigma[i]}, the value of NRMSE is: {sigma_impact[i]}\")"
      ],
      "metadata": {
        "id": "Vh4RIMltj_wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLotting the graph between Variance vs. value of NRMSE.\n",
        "plt.plot(sigma, sigma_impact)\n",
        "plt.title('Impact of sigma on NRMSE')\n",
        "plt.xlabel('sigma')\n",
        "plt.ylabel('Value of NRMSE after test')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "A_14nXgarJ3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10."
      ],
      "metadata": {
        "id": "65mMX-k96U3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_values = [10, 100, 1000, 10000, 100000] # Array initialising the number of samples values.\n",
        "\n",
        "lambda2_values = [0.001,0.01,0.1,1.0,10.0] # Array initialising the lambda2 values.\n",
        "\n",
        "sigma = 0.3\n",
        "\n",
        "# The below loop stores the averaged out NRMSE values over 5 iterations for various N and sigma pairs.\n",
        "for i in range(5):\n",
        "  nmrse = 0\n",
        "  for j in range(5):\n",
        "    w = np.ones((D+1,1))\n",
        "    x_q10 = generate_input_data(N_values[j], M, D)\n",
        "    t_q10 = gen_target_vector(x_q10, w[:-1], sigma)\n",
        "    t, t1 = NRMSE(x_q10, t_q10, w[:-1], 1e-2, 1e-1, lambda2_values[i], 10000,1e-5)\n",
        "    nmrse += t1\n",
        "\n",
        "  print(f\"For lambda2 values {lambda2_values[i]}, the averaged out NRMSE values is {nmrse/5}\")"
      ],
      "metadata": {
        "id": "ZP0zZEDFLKxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 11."
      ],
      "metadata": {
        "id": "v-6SnMFs6ZxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lambda1_values = np.array([0.001, 0.01, 0.1, 1.0, 10.0]) #  Array initialising the lambda2 values.\n",
        "\n",
        "# Defining the correlation matrix.\n",
        "correlation_matrix = np.array([[1, 0.8, 0.5, 0.2, 0.1],\n",
        "                               [0.8, 1, 0.7, 0.3, 0.2],\n",
        "                               [0.5, 0.7, 1, 0.4, 0.3],\n",
        "                               [0.2, 0.3, 0.4, 1, 0.6],\n",
        "                               [0.1, 0.2, 0.3, 0.6, 1]])\n",
        "\n",
        "\n",
        "weights_list = []\n",
        "for i in range(5):\n",
        "    w = np.ones((D+1, 1))\n",
        "    x_q11 = generate_input_data(N, M, D)\n",
        "    t_q11 = gen_target_vector(x_q11, w[:-1], sigma)\n",
        "\n",
        "    # Transform X to make it correlated using the Cholesky decomposition.\n",
        "    X_correlated = X.dot(np.linalg.cholesky(correlation_matrix).T)\n",
        "    t, t1 = grad_descent(X_correlated, t_q11, w[:-1], 1e-2, lambda1_values, 1e-1, 10000,1e-5)\n",
        "\n",
        "    weights_list.append(t)\n",
        "\n",
        "# print(f\"Shape of x: {lambda1_values.shape}\")\n",
        "# print(f\"Shape of y: {weights_list.shape}\")\n",
        "\n",
        "\n",
        "weights_array = np.array(weights_list)\n",
        "# weights_array = weights_array.reshape(-1,)\n",
        "\n",
        "# lambda1_values = lambda1_values.reshape((5, 1))\n",
        "for i in range(weights_array.shape[1]):\n",
        "    plt.plot(1 / lambda1_values, weights_array[:, i], label=f'Weight {i+1}', marker = 'o')\n",
        "    # plt.plot([1 / lambda1 for lambda1 in lambda1_values], [weights[i] for weights in weights_list], label=f'Weight {i+1}')\n",
        "    # plt.plot(1 / lambda1_values, weights_array[:, i], label=f'Weight {i+1}')\n",
        "\n",
        "plt.xscale('log')  # Set x-axis to log scale\n",
        "plt.xlabel('1/lambda1')\n",
        "plt.ylabel('Weights')\n",
        "plt.title('Weights vs. 1/lambda for Linear Regression')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jJKXbuOYOsdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 12."
      ],
      "metadata": {
        "id": "ZF_M0jeggYNz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing elastic net and standard scaler function from sklearn.\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "xx = generate_input_data(N, M, D)\n",
        "\n",
        "tt = gen_target_vector(x_q11, w[:-1], sigma)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_standardized = scaler.fit_transform(xx)\n",
        "\n",
        "# Apply Elastic Net with varying alpha values\n",
        "alpha_values = [0.1, 0.63, 0.55, 0.87]\n",
        "weights_list = []\n",
        "\n",
        "for alpha in alpha_values:\n",
        "    elastic_net = ElasticNet(alpha=1, l1_ratio = alpha)\n",
        "    elastic_net.fit(xx, tt)\n",
        "    weights_list.append(elastic_net.coef_)\n",
        "\n",
        "# Convert to numpy array for easier manipulation\n",
        "weights_array = np.array(weights_list)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(weights_array.shape[1]):\n",
        "    plt.plot(weights_array[:, i], label=f'Feature {i+1}')\n",
        "\n",
        "plt.xlabel('Alpha (Mixing parameter)')\n",
        "plt.ylabel('Weights')\n",
        "plt.title('Grouping Effect of Elastic Net on Correlated Columns')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vuQpprBpQ9dT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 13.\n"
      ],
      "metadata": {
        "id": "UgQZGo_YhwhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_bin_class(X, w, b, sigma):\n",
        "  \"\"\"\n",
        "  This funcion returns the binary classification array using the inputs:\n",
        "  X: input data matrix\n",
        "  w: weights vectos\n",
        "  b: bias\n",
        "  sigma: variance\n",
        "  \"\"\"\n",
        "  linear_part = np.dot(X, w) + b # Calculates the linear part of the prediction matrix.\n",
        "\n",
        "  t = np.sign(linear_part + np.random.normal(0, np.sqrt(sigma), size = (X.shape[0], 1))) #Calculates the target vector\n",
        "\n",
        "  return t\n",
        "\n",
        "X = generate_input_data(N, M ,D)\n",
        "w = np.ones((D+1, 1))\n",
        "b = 0.2\n",
        "vec = linear_bin_class(X, w[:-1], b, sigma)\n",
        "\n",
        "print(vec)\n"
      ],
      "metadata": {
        "id": "eF7DM2BGhGQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 14."
      ],
      "metadata": {
        "id": "8CakhG_KjCWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid_func(z):\n",
        "  \"\"\"\n",
        "  This function returns the sigmoid values using the input:\n",
        "  z: prediction matrix\n",
        "  \"\"\"\n",
        "\n",
        "  return 1/(1 + np.exp(-z))\n",
        "\n",
        "def cross_entropy(X, target_vector, w, b):\n",
        "  \"\"\"\n",
        "  This function returns the gradients of weights and bias using inputs:\n",
        "  X: input data matrix\n",
        "  target_vector: vector containing actual Y-values\n",
        "  w: weights vector\n",
        "  b: bias\n",
        "  \"\"\"\n",
        "\n",
        "  m = len(target_vector)\n",
        "\n",
        "  pred_y = sigmoid_func(X.dot(w) + b)\n",
        "\n",
        "  weight_gradients = X.T.dot(pred_y - target_vector)/m\n",
        "\n",
        "  gradient_bias = np.sum(pred_y - target_vector)/m\n",
        "\n",
        "  return weight_gradients, gradient_bias\n",
        "\n",
        "\n",
        "X = generate_input_data(N, M, D)\n",
        "\n",
        "target_vector = gen_target_vector(X, w[:-1], sigma)\n",
        "\n",
        "bias = 2\n",
        "\n",
        "gradient_of_weights, gradient_bias = cross_entropy(X, target_vector, w[:-1], bias)\n",
        "\n",
        "print(f\"Gradient of weights: {gradient_of_weights}\")\n",
        "print(f\"Gradient of bias: {gradient_bias}\")"
      ],
      "metadata": {
        "id": "DPZF-Lelixip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def q15(x,t,w,eta,labda_1, labda_2, max_iter, min_change_NRMSE):\n",
        "  \"\"\"\n",
        "  This function returns the updated weights vector and updated MSE value after iterating it through multiple times to get the NRMSE error below\n",
        "  a threshold value which implies a close-to-zero slope using the inputs:\n",
        "  x: input data matrix\n",
        "  t: target vector containing the actual Y-values\n",
        "  w: weights vector\n",
        "  alpha: learning rate\n",
        "  lambda1 and lambda2: regularisation parameter\n",
        "  max_iter: maximum number of iterations\n",
        "  min_change_NRMSE: the threshold NRMSE value below which the loop stops and we reach a close-to-zero slope.\n",
        "  \"\"\"\n",
        "  mse_new = 100 # Dummy\n",
        "  wts_new = w.copy() # Makes a copy of the updated weights after every iteration.\n",
        "  std_t = np.std(t) # Standard deviation for t matrix, for nrmse calculation.\n",
        "  nrmse_old = 0\n",
        "  diff = 0\n",
        "\n",
        "  for i in range(max_iter): # Loops around for maximum iterations\n",
        "    if diff <= min_change_NRMSE and i > 1: # s\\Stops loop if difference between last and current NRMSE is less than the threshold value.\n",
        "      break\n",
        "    final_wts, mse_new = cross_entropy(X, target_vector, w[:-1], bias)\n",
        "    nrmse_new = np.sqrt(mse_new) / std_t\n",
        "    diff = abs(nrmse_new - nrmse_old)\n",
        "    nrmse_old = nrmse_new\n",
        "\n",
        "\n",
        "  return final_wts, nrmse_new"
      ],
      "metadata": {
        "id": "0ZkN6VwD5rqM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 15."
      ],
      "metadata": {
        "id": "OzE1vdocmrK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Repeats question 10 using binary classification.\n",
        "N_values = [10, 100, 1000, 10000, 100000]\n",
        "\n",
        "lambda2_values = [0.001,0.01,0.1,1.0,10.0]\n",
        "\n",
        "sigma = 0.3\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "  nmrse = 0\n",
        "  for j in range(5):\n",
        "    w = np.ones((D+1,1))\n",
        "    x_q10 = generate_input_data(N_values[j], M, D)\n",
        "    t_q10 = gen_target_vector(x_q10, w[:-1], sigma)\n",
        "\n",
        "    t, t1 = q15(x_q10, t_q10, w, 1e-2, 1e-1, lambda2_values[i], 10000,1e-5)\n",
        "    nmrse += t1\n",
        "\n",
        "\n",
        "  print(f\"For lambda2 values {lambda2_values[i]}, the averaged out NRMSE values is {nmrse/5}\")"
      ],
      "metadata": {
        "id": "bTMWjyFa07cz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only help I took was from ChatGPT, everything else was done solely by me."
      ],
      "metadata": {
        "id": "0Nsz8LSYDio6"
      }
    }
  ]
}